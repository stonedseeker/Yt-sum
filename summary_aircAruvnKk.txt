A neural network can learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic. The network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image. The activation in these neurons, again, some number that's between zero and one, represents how much the system thinks a given image corresponds with a given digit. The way the network operates, activations in one layer determine the activations of the next layer. It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. The brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. The network has almost exactly 13,000 total weights and biases. The weights tell you what pixel pattern this neuron in the second layer is picking up on. The bias tells you how high the weighted sum needs to be, before the neuron starts getting meaningfully active. The entire network is just a function, one that takes in 784 numbers as an input and spits out 10 as an output. How does this network learn the appropriate weights and biases just by looking at data? Oh, that's what I'll show in the next video. Relu stands for rectified linear unit. It's this kind of function where you're just taking a max of zero and A, where A is given by what you were explaining in the video. Relu was sort of motivated from, I think, was a partially biobiological analogy with how neurons would either be activated or not.